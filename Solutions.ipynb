{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UbFxS0sUs9-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "import random \n",
    "from statistics import mean\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccZ9_KvsU5nI"
   },
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czeGbR4-U87E"
   },
   "source": [
    "#### 1i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xstERzQAUs9_",
    "outputId": "dad9970e-11ca-4b80-dbe3-f842e8daea7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of implementing the  2 -NN algorithm and a  5 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  4 -NN algorithm and a  5 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  6 -NN algorithm and a  5 -fold cross validation is:  95.33333333333334 %\n",
      "Accuracy of implementing the  8 -NN algorithm and a  5 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  2 -NN algorithm and a  10 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  4 -NN algorithm and a  10 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  6 -NN algorithm and a  10 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  8 -NN algorithm and a  10 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  2 -NN algorithm and a  15 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  4 -NN algorithm and a  15 -fold cross validation is:  96.0 %\n",
      "Accuracy of implementing the  6 -NN algorithm and a  15 -fold cross validation is:  96.66666666666667 %\n",
      "Accuracy of implementing the  8 -NN algorithm and a  15 -fold cross validation is:  97.33333333333334 %\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Class']\n",
    "iris_data = pd.read_csv('UCIdata-exercise1/iris.data', names = col_names)\n",
    "\n",
    "def knn(data, query, k) : #data will be in the form of a DataFrame with the first columns been used to calculate the nearest \n",
    "    #neighbor and the last one will be used for the classification of each element\n",
    "    neighbor_distances_and_indices = []\n",
    "    # 1. For each example in the data\n",
    "    for index in range(len(data)):\n",
    "        # 1.1 Calculate the distance between the query example and the current\n",
    "        # example from the data.\n",
    "        A=data.loc[index].tolist()[:-1] #transform to list the values of the dimensions of every instance of the DataFrame\n",
    "        distance1 = distance.euclidean(A, query)\n",
    "        \n",
    "        \n",
    "        # 1.2 Add the distance and the index of the example to an ordered collection\n",
    "        neighbor_distances_and_indices.append((distance1, index))\n",
    "    \n",
    "    # 2. Sort the ordered collection of distances and indices from\n",
    "    # smallest to largest (in ascending order) by the distances\n",
    "    sorted_neighbor_distances_and_indices = sorted(neighbor_distances_and_indices)\n",
    "    \n",
    "    # 3. Pick the first K entries from the sorted collection\n",
    "    k_nearest_distances_and_indices = sorted_neighbor_distances_and_indices[:k]\n",
    "    \n",
    "    # 4. Get the labels of the selected K entries\n",
    "    k_nearest_labels = [data.loc[i][-1] for distance, i in k_nearest_distances_and_indices]\n",
    "\n",
    "    return Counter(k_nearest_labels).most_common(1)[0][0] #, k_nearest_distances_and_indices \n",
    "\n",
    "def partition (df, n): # in order to perform k - fold validation we split the dataframe into n - random dataframes \n",
    "#and create a list containing them\n",
    "    df=df.sample(frac = 1)\n",
    "    cols=df.columns.tolist()\n",
    "    split_df=[]\n",
    "    n1=0\n",
    "    n2=len(df)//n\n",
    "    if len(df)//n==0:\n",
    "        for i in range(n):\n",
    "            split_df.append(df.iloc[n1:n2])\n",
    "            n1=n2\n",
    "            n2=n2+len(df)//n\n",
    "        return(split_df)\n",
    "    else:\n",
    "        residual=len(df)%n\n",
    "        for i in range(n):\n",
    "            if n-i-1>residual:\n",
    "                split_df.append(df.iloc[n1:n2])\n",
    "                n1=n2\n",
    "                n2=n2+len(df)//n\n",
    "            else:\n",
    "                split_df.append(df.iloc[n1:n2])\n",
    "                n1=n2\n",
    "                n2=n2+len(df)//n+1\n",
    "        return(split_df)\n",
    "            \n",
    "\n",
    "#We will implement 5 -fold, 10 -fold and 15 -fold validation to the iris dataset. The k for each time implementing the knn will \n",
    "#be an even number each time, since there are 3 different target Classes (odd number of classes). So k will be 2,4,6 or 8\n",
    "#each time\n",
    "\n",
    "for n_fold in ([5,10,15]):\n",
    "    splited=partition(iris_data,n_fold)\n",
    "    for k in range(2,9,2):\n",
    "        accuracy_list=[]\n",
    "        for i in range(len(splited)):\n",
    "            splited_2=splited[:]\n",
    "            test_data=splited_2.pop(i)\n",
    "            train_data=pd.concat(splited_2)\n",
    "            train_data.index=range(len(train_data))\n",
    "            counter_accuracy=0\n",
    "            for j in range(len(test_data)):\n",
    "                if knn(train_data,test_data.iloc[j][:4].tolist(),k)==test_data.iloc[j][4]:\n",
    "                    counter_accuracy+=1\n",
    "            accuracy=counter_accuracy/len(test_data)\n",
    "            accuracy_list.append(accuracy)\n",
    "        final_accuracy=mean(accuracy_list)\n",
    "        print ('Accuracy of implementing the ',k,'-NN algorithm and a ',n_fold,'-fold cross validation is: ',100*final_accuracy,'%')\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd4ZV8kTVA-7"
   },
   "source": [
    "#### 1ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96LyrokFUs-B"
   },
   "source": [
    "So the best Algorithm to be implemented in the iris dataset is the 8 NN, since it has averaged a 96,44 % accuracy in 10-fold, 20- fold and 30-fold cross valdiation respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ELVfkToUs-C",
    "outputId": "1a5d6bf8-1f8e-46ab-b31f-9353140e8f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of implementing the  5 -NN algorithm and a  10 -fold cross validation is:  72.00615174299385 %\n",
      "Accuracy of implementing the  9 -NN algorithm and a  10 -fold cross validation is:  73.95591250854409 %\n",
      "Accuracy of implementing the  13 -NN algorithm and a  10 -fold cross validation is:  73.69104579630896 %\n",
      "Accuracy of implementing the  17 -NN algorithm and a  10 -fold cross validation is:  73.69958988380041 %\n",
      "Accuracy of implementing the  5 -NN algorithm and a  20 -fold cross validation is:  70.68825910931174 %\n",
      "Accuracy of implementing the  9 -NN algorithm and a  20 -fold cross validation is:  73.5425101214575 %\n",
      "Accuracy of implementing the  13 -NN algorithm and a  20 -fold cross validation is:  73.5391363022942 %\n",
      "Accuracy of implementing the  17 -NN algorithm and a  20 -fold cross validation is:  73.27935222672065 %\n",
      "Accuracy of implementing the  5 -NN algorithm and a  30 -fold cross validation is:  71.9025641025641 %\n",
      "Accuracy of implementing the  9 -NN algorithm and a  30 -fold cross validation is:  73.86153846153847 %\n",
      "Accuracy of implementing the  13 -NN algorithm and a  30 -fold cross validation is:  74.1076923076923 %\n",
      "Accuracy of implementing the  17 -NN algorithm and a  30 -fold cross validation is:  74.50256410256411 %\n"
     ]
    }
   ],
   "source": [
    "col_names = ['Pregnancies','Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age','Class']\n",
    "pima_data = pd.read_csv('UCIdata-exercise1/pima-indians-diabetes.data', names = col_names)\n",
    "\n",
    "# There are values in  columns that represent health tests  that should have value >0 such as Insulin.\n",
    "#These values will be replaced by the average of the column they are included\n",
    "for col in ['Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age']:\n",
    "    val = pima_data[col].mean()\n",
    "    pima_data[col] = pima_data[col].replace(0, val)\n",
    "    \n",
    "#We will implement  10 -fold, 20 -fold and 30-fold validation to the pima dataset. The k for each time implementing the knn \n",
    "#will be an odd number each time, since there are 2 different target Classes (even number of classes). \n",
    "#So k in that case will be 5,9,13,17 each time\n",
    "\n",
    "\n",
    "#Since the dataset is larger that the previous one (iris), we will create another function that will accept as input a dataframe\n",
    "# and a list with values of the dimensions represented in that dataframe. As a result, it will calculate all the euclidean \n",
    "#distances between all the elements of the dataframe and the list, and it will reuturn a list of all the sorted distances, \n",
    "#and at what row of the dataframe each distance occurs. In that way,for each N-fold validation, we will get a sorted number of \n",
    "#the distances, and later we will extract the first 5,9,13 or 17 elements, depending on whether we are implementing the \n",
    "#5-NN, 9-NN,13-NN or 13-NN algorithm\n",
    "\n",
    "\n",
    "def euclidean_1(data, query) : #data will be in the form of a DataFrame with the first columns been used to calculate the\n",
    "    #nearest  neighbor and the last one will be used for the classification of each element\n",
    "   \n",
    "    # it returns the sorted euclidean distance as well as the index of the row of the dataframe data in which the smaller \n",
    "    #euclidean distance to the list \"query\" ,which represents a list of dimenions' values of a datapoint , occurs\n",
    "    \n",
    "    neighbor_distances_and_indices = []\n",
    "    # 1. For each example in the data\n",
    "    for index in range(len(data)):\n",
    "        # 1.1 Calculate the distance between the query example and the current\n",
    "        # example from the data.\n",
    "        A=data.loc[index].tolist()[:-1] #transform to list the values of the dimensions of every instance of the DataFrame\n",
    "        distance1 = distance.euclidean(A, query)\n",
    "        \n",
    "        \n",
    "        # 1.2 Add the distance and the index of the example to an ordered collection\n",
    "        neighbor_distances_and_indices.append((distance1, index))\n",
    "    \n",
    "    # 2. Sort the ordered collection of distances and indices from\n",
    "    # smallest to largest (in ascending order) by the distances\n",
    "    return (sorted(neighbor_distances_and_indices))\n",
    "    \n",
    "\n",
    "for n_fold in ([10,20,30]): #10-fold, 20-fold or 30-fold cross validation\n",
    "    splited=partition(pima_data,n_fold)\n",
    "    \n",
    "    accuracy_list_5=[] #this list will be filled with the accuracies of the 5-NN algorithm, for each of the \n",
    "    # N-fold cross validations each time\n",
    "    accuracy_list_9=[] #the same as above\n",
    "    accuracy_list_13=[]\n",
    "    accuracy_list_17=[]\n",
    "    for i in range(len(splited)):\n",
    "        splited_2=splited[:]\n",
    "        test_data=splited_2.pop(i)\n",
    "        train_data=pd.concat(splited_2)\n",
    "        train_data.index=range(len(train_data))\n",
    "        counter_accuracy_5=0\n",
    "        counter_accuracy_9=0\n",
    "        counter_accuracy_13=0\n",
    "        counter_accuracy_17=0\n",
    "        for j in range(len(test_data)):\n",
    "            distance2=euclidean_1(train_data,test_data.iloc[j][:8].tolist())\n",
    "            #5-nn calculations\n",
    "            _5_nearest_distances_and_indices = distance2[:5]\n",
    "            _5_nearest_labels = [train_data.loc[i][-1] for distance, i in _5_nearest_distances_and_indices]\n",
    "            if Counter(_5_nearest_labels).most_common(1)[0][0]==test_data.iloc[j][8]:\n",
    "                counter_accuracy_5+=1\n",
    "                \n",
    "            #9-nn\n",
    "            _9_nearest_distances_and_indices = distance2[:9]\n",
    "            _9_nearest_labels = [train_data.loc[i][-1] for distance, i in _9_nearest_distances_and_indices]\n",
    "            if Counter(_9_nearest_labels).most_common(1)[0][0]==test_data.iloc[j][8]:\n",
    "                counter_accuracy_9+=1\n",
    "                \n",
    "            #13-nn\n",
    "            _13_nearest_distances_and_indices = distance2[:13]\n",
    "            _13_nearest_labels = [train_data.loc[i][-1] for distance, i in _13_nearest_distances_and_indices]\n",
    "            if Counter(_13_nearest_labels).most_common(1)[0][0]==test_data.iloc[j][8]:\n",
    "                counter_accuracy_13+=1\n",
    "                \n",
    "            #17-nn\n",
    "            _17_nearest_distances_and_indices = distance2[:17]\n",
    "            _17_nearest_labels = [train_data.loc[i][-1] for distance, i in _17_nearest_distances_and_indices]\n",
    "            if Counter(_17_nearest_labels).most_common(1)[0][0]==test_data.iloc[j][8]:\n",
    "                counter_accuracy_17+=1\n",
    "                \n",
    "        accuracy_list_5.append(counter_accuracy_5/len(test_data))\n",
    "        accuracy_list_9.append(counter_accuracy_9/len(test_data))\n",
    "        accuracy_list_13.append(counter_accuracy_13/len(test_data))\n",
    "        accuracy_list_17.append(counter_accuracy_17/len(test_data))\n",
    "    print ('Accuracy of implementing the ',5,'-NN algorithm and a ',n_fold,'-fold cross validation is: ',100*mean(accuracy_list_5),'%')    \n",
    "    print ('Accuracy of implementing the ',9,'-NN algorithm and a ',n_fold,'-fold cross validation is: ',100*mean(accuracy_list_9),'%')    \n",
    "    print ('Accuracy of implementing the ',13,'-NN algorithm and a ',n_fold,'-fold cross validation is: ',100*mean(accuracy_list_13),'%')    \n",
    "    print ('Accuracy of implementing the ',17,'-NN algorithm and a ',n_fold,'-fold cross validation is: ',100*mean(accuracy_list_17),'%')    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3jyXpSgUs-D"
   },
   "source": [
    "So the best Algorithm to be implemented in the pima dataset is the 17 NN, since it has averaged a 73.82 % accuracy in 10-fold, 20- fold and 30-fold cross valdiation respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFsf2jyjVElc"
   },
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFyU7GdBVGjM"
   },
   "source": [
    "#### 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35AVDpnuUs-D"
   },
   "outputs": [],
   "source": [
    "# Firstly for the iris data\n",
    "col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Class']\n",
    "iris_data = pd.read_csv('UCIdata-exercise1/iris.data', names = col_names)\n",
    "\n",
    "#firstly we will seperate 3 classes and its respective features. We will create 3 different arrays. Each element of every array\n",
    "#will include 4 dimensions\n",
    "setosa=iris_data[iris_data[\"Class\"] == 'Iris-setosa'].drop([\"Class\"], axis=1).to_numpy()\n",
    "versicolor=iris_data[iris_data[\"Class\"] == 'Iris-versicolor'].drop([\"Class\"], axis=1).to_numpy()\n",
    "virginica=iris_data[iris_data[\"Class\"] == 'Iris-virginica'].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "\n",
    "#Since for every class the Covariance matrices are diagonal, with all diagonal elements equal  one can assume to have \n",
    "#only a diagonal covariance matrix and one can estimate \n",
    "#the mean and the variance in each dimension separately and describe the multivariate density function in terms of a \n",
    "#product of univariate Gaussians. Moreover, since it is impossible for all 4 variables to have the same variance for each case\n",
    "#each element of the diagonal covariance matrix will be the the maximum of the four variances (worst case scenario)\n",
    "\n",
    "\n",
    "# setosa Class\n",
    "Φ=setosa\n",
    "mean11=np.sum(Φ,axis=0)/setosa.shape[0]\n",
    "var=0\n",
    "for column in range(setosa.shape[1]):\n",
    "    if np.var(setosa[:,column])>var:\n",
    "        var=np.var(setosa[:,column])\n",
    "Σ11=np.identity(setosa.shape[1])*var    #calculate the covariaance matrix\n",
    "def setosa_pdf_1 (x): #x be a 4-dimensional vector\n",
    "    pdf= multivariate_normal(mean11,Σ11).pdf(x)\n",
    "    return pdf\n",
    "\n",
    "# versicolor Class\n",
    "Φ=versicolor\n",
    "mean21=np.sum(Φ,axis=0)/versicolor.shape[0] \n",
    "var=0\n",
    "for column in range(versicolor.shape[1]):\n",
    "    if np.var(versicolor[:,column])>var:\n",
    "        var=np.var(versicolor[:,column])\n",
    "Σ21=np.identity(versicolor.shape[1])*var    #calculate the covariaance matrix\n",
    "def versicolor_pdf_1 (x): #x be a 4-dimensional vector\n",
    "    pdf= multivariate_normal(mean21,Σ21).pdf(x)\n",
    "    return pdf\n",
    "\n",
    "# virginica Class\n",
    "Φ=virginica\n",
    "mean31=np.sum(Φ,axis=0)/virginica.shape[0] \n",
    "var=0\n",
    "for column in range(virginica.shape[1]):\n",
    "    if np.var(virginica[:,column])>var:\n",
    "        var=np.var(virginica[:,column])\n",
    "Σ31=np.identity(virginica.shape[1])*var    #calculate the covariaance matrix\n",
    "def virginica_pdf_1 (x): #x be a 4-dimensional vector\n",
    "    pdf= multivariate_normal(mean31,Σ31).pdf(x)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLj86hBqUs-E"
   },
   "outputs": [],
   "source": [
    "# Then for the pima data\n",
    "col_names = ['Pregnancies','Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age','Class']\n",
    "pima_data = pd.read_csv('UCIdata-exercise1/pima-indians-diabetes.data', names = col_names)\n",
    "for col in ['Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age']:\n",
    "    val = pima_data[col].mean()\n",
    "    pima_data[col] = pima_data[col].replace(0, val)\n",
    "\n",
    "#firstly we will seperate 2 classes and its respective features. We will create 2 different arrays. Each element of every array\n",
    "#will include 8 dimensions\n",
    "yes_diabetes=pima_data[pima_data[\"Class\"] == 1].drop([\"Class\"], axis=1).to_numpy()\n",
    "no_diabetes=pima_data[pima_data[\"Class\"] == 0].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "#yes_diabetes\n",
    "Φ=yes_diabetes\n",
    "mean41=np.sum(Φ,axis=0)/yes_diabetes.shape[0] #case I.I.D dimensions #case I.I.D dimensions\n",
    "var=0\n",
    "for column in range(yes_diabetes.shape[1]):\n",
    "    if np.var(yes_diabetes[:,column])>var:\n",
    "        var=np.var(yes_diabetes[:,column])\n",
    "Σ41=np.identity(yes_diabetes.shape[1])*var    #calculate the covariaance matrix\n",
    "def yes_diabetes_pdf_1 (x): #x be a 8-dimensional vector\n",
    "    pdf= multivariate_normal(mean41,Σ41).pdf(x)\n",
    "    return pdf\n",
    "\n",
    "#no_diabetes\n",
    "Φ=no_diabetes\n",
    "mean51=np.sum(Φ,axis=0)/no_diabetes.shape[0] \n",
    "var=0\n",
    "for column in range(no_diabetes.shape[1]):\n",
    "    if np.var(no_diabetes[:,column])>var:\n",
    "        var=np.var(no_diabetes[:,column])\n",
    "Σ51=np.identity(no_diabetes.shape[1])*var    #calculate the covariaance matrix\n",
    "def no_diabetes_pdf_1 (x): #x be a 8-dimensional vector\n",
    "    pdf= multivariate_normal(mean51,Σ51).pdf(x)\n",
    "    return pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxTnbzsCVPU9"
   },
   "source": [
    "#### 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a52gYuXEUs-F"
   },
   "outputs": [],
   "source": [
    "# For the purpose of solution of 2)b we consider the variables to be idependant and identically distributed\n",
    "# Firstly for the iris data \n",
    "col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Class']\n",
    "iris_data = pd.read_csv('UCIdata-exercise1/iris.data', names = col_names)\n",
    "\n",
    "#firstly we will seperate 3 classes and its respective features. We will create 3 different arrays. Each element of every array\n",
    "#will include 4 dimensions\n",
    "setosa=iris_data[iris_data[\"Class\"] == 'Iris-setosa'].drop([\"Class\"], axis=1).to_numpy()\n",
    "versicolor=iris_data[iris_data[\"Class\"] == 'Iris-versicolor'].drop([\"Class\"], axis=1).to_numpy()\n",
    "virginica=iris_data[iris_data[\"Class\"] == 'Iris-virginica'].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "# setosa\n",
    "Φ=setosa\n",
    "mean12=np.sum(Φ,axis=0)/setosa.shape[0] #case I.I.D dimensions\n",
    "sum_=0\n",
    "for i in range(setosa.shape[0]):\n",
    "    sum_+=np.matmul((Φ[i]-mean12).reshape(4,1),np.transpose((Φ[i]-mean12).reshape(4,1)))\n",
    "Σ12=sum_/setosa.shape[0]\n",
    "def setosa_pdf_2 (x): #x be a 4-dimensional vector\n",
    "    pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean12).reshape(4,1)),np.linalg.inv(Σ12)),(x-mean12).reshape(4,1)))/(((2*math.pi)**(setosa.shape[1]/2))*(np.linalg.det(Σ12))**(1/2))\n",
    "    return pdf\n",
    "\n",
    "# versicolor\n",
    "Φ=versicolor\n",
    "mean22=np.sum(Φ,axis=0)/versicolor.shape[0] #case I.I.D dimensions\n",
    "sum_=0\n",
    "for i in range(versicolor.shape[0]):\n",
    "    sum_+=np.matmul((Φ[i]-mean22).reshape(4,1),np.transpose((Φ[i]-mean22).reshape(4,1)))\n",
    "Σ22=sum_/versicolor.shape[0]\n",
    "def versicolor_pdf_2 (x): #x be a 4-dimensional vector\n",
    "    pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean22).reshape(4,1)),np.linalg.inv(Σ22)),(x-mean22).reshape(4,1)))/(((2*math.pi)**(versicolor.shape[1]/2))*(np.linalg.det(Σ22))**(1/2))\n",
    "    return pdf\n",
    "\n",
    "# virginica\n",
    "Φ=virginica\n",
    "mean32=np.sum(Φ,axis=0)/virginica.shape[0] #case I.I.D dimensions\n",
    "sum_=0\n",
    "for i in range(virginica.shape[0]):\n",
    "    sum_+=np.matmul((Φ[i]-mean32).reshape(4,1),np.transpose((Φ[i]-mean32).reshape(4,1)))\n",
    "Σ32=sum_/virginica.shape[0]\n",
    "def virginica_pdf_2 (x): #x be a 4-dimensional vector\n",
    "    pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean32).reshape(4,1)),np.linalg.inv(Σ32)),(x-mean32).reshape(4,1)))/(((2*math.pi)**(virginica.shape[1]/2))*(np.linalg.det(Σ32))**(1/2))\n",
    "    return pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFZFa8ngUs-F"
   },
   "outputs": [],
   "source": [
    "# For the purpose of solution of 2)b we consider the variables to be idependant and identically distributed\n",
    "# Then for the pima data\n",
    "col_names = ['Pregnancies','Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age','Class']\n",
    "pima_data = pd.read_csv('UCIdata-exercise1/pima-indians-diabetes.data', names = col_names)\n",
    "for col in ['Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age']:\n",
    "    val = pima_data[col].mean()\n",
    "    pima_data[col] = pima_data[col].replace(0, val)\n",
    "\n",
    "#firstly we will seperate 2 classes and its respective features. We will create 2 different arrays. Each element of every array\n",
    "#will include 8 dimensions\n",
    "yes_diabetes=pima_data[pima_data[\"Class\"] == 1].drop([\"Class\"], axis=1).to_numpy()\n",
    "no_diabetes=pima_data[pima_data[\"Class\"] == 0].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "#yes_diabetes\n",
    "Φ=yes_diabetes\n",
    "mean42=np.sum(Φ,axis=0)/yes_diabetes.shape[0] #case I.I.D dimensions\n",
    "sum_=0\n",
    "for i in range(yes_diabetes.shape[0]):\n",
    "    sum_+=np.matmul((Φ[i]-mean42).reshape(8,1),np.transpose((Φ[i]-mean42).reshape(8,1)))\n",
    "Σ42=sum_/yes_diabetes.shape[0]\n",
    "def yes_diabetes_pdf_2 (x): #x be a 8-dimensional vector\n",
    "    pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean42).reshape(8,1)),np.linalg.inv(Σ42)),(x-mean42).reshape(8,1)))/(((2*math.pi)**(yes_diabetes.shape[1]/2))*(np.linalg.det(Σ42))**(1/2))\n",
    "    return pdf\n",
    "\n",
    "#no_diabetes\n",
    "Φ=no_diabetes\n",
    "mean52=np.sum(Φ,axis=0)/no_diabetes.shape[0] #case I.I.D dimensions\n",
    "sum_=0\n",
    "for i in range(no_diabetes.shape[0]):\n",
    "    sum_+=np.matmul((Φ[i]-mean52).reshape(8,1),np.transpose((Φ[i]-mean52).reshape(8,1)))\n",
    "Σ52=sum_/no_diabetes.shape[0]\n",
    "def no_diabetes_pdf_2 (x): #x be a 8-dimensional vector\n",
    "    pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean52).reshape(8,1)),np.linalg.inv(Σ52)),(x-mean52).reshape(8,1)))/(((2*math.pi)**(no_diabetes.shape[1]/2))*(np.linalg.det(Σ52))**(1/2))\n",
    "    return pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK7aLHftVVHh"
   },
   "source": [
    "#### 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJuilEqhUs-G"
   },
   "outputs": [],
   "source": [
    "# PDF for the Naive Bayes Approach\n",
    "def pdf_naive(x,mean,variance): #mean and variance list of the means and variances of the same length- same as the no of the \n",
    "#variables , x an 1-d array of the values of the variables of the same length of the above lists\n",
    "    pdf=1\n",
    "    for count in range(len(mean)):\n",
    "        pdf=(pdf*math.exp((-1/2)*(1/variance[count])*(x[count]-mean[count])**2))/((2*math.pi)**(1/2)*((variance[count])**(1/2)))\n",
    "    return pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZrGctWKUs-G"
   },
   "outputs": [],
   "source": [
    "# Firstly for the iris data \n",
    "\n",
    "col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Class']\n",
    "iris_data = pd.read_csv('UCIdata-exercise1/iris.data', names = col_names)\n",
    "\n",
    "#firstly we will seperate 3 classes and its respective features. We will create 3 different arrays. Each element of every array\n",
    "#will include 4 dimensions\n",
    "setosa=iris_data[iris_data[\"Class\"] == 'Iris-setosa'].drop([\"Class\"], axis=1).to_numpy()\n",
    "versicolor=iris_data[iris_data[\"Class\"] == 'Iris-versicolor'].drop([\"Class\"], axis=1).to_numpy()\n",
    "virginica=iris_data[iris_data[\"Class\"] == 'Iris-virginica'].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "# setosa\n",
    "Φ=setosa\n",
    "mean13=(np.sum(Φ,axis=0)/setosa.shape[0]).tolist() #case I.I.D dimensions\n",
    "Σ13=[]\n",
    "for column in range(setosa.shape[1]):\n",
    "    Σ13.append(np.var(setosa[:,column]))\n",
    "\n",
    "def setosa_pdf_3 (x): #x be a 4-dimensional vector\n",
    "    pdf= pdf_naive(x,mean13,Σ13)\n",
    "    return pdf\n",
    "\n",
    "#versicolor\n",
    "Φ=versicolor\n",
    "mean23=(np.sum(Φ,axis=0)/versicolor.shape[0]).tolist() #case I.I.D dimensions\n",
    "Σ23=[]\n",
    "for column in range(versicolor.shape[1]):\n",
    "    Σ23.append(np.var(versicolor[:,column]))\n",
    "\n",
    "def versicolor_pdf_3 (x): #x be a 4-dimensional vector\n",
    "    pdf= pdf_naive(x,mean23,Σ23)\n",
    "    return pdf\n",
    "\n",
    "#virginica\n",
    "Φ=virginica\n",
    "mean33=(np.sum(Φ,axis=0)/virginica.shape[0]).tolist() #case I.I.D dimensions\n",
    "Σ33=[]\n",
    "for column in range(virginica.shape[1]):\n",
    "    Σ33.append(np.var(virginica[:,column]))\n",
    "\n",
    "def virginica_pdf_3 (x): #x be a 4-dimensional vector\n",
    "    pdf= pdf_naive(x,mean33,Σ33)\n",
    "    return pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62xldktYUs-G"
   },
   "outputs": [],
   "source": [
    "# Then for the pima data\n",
    "col_names = ['Pregnancies','Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age','Class']\n",
    "pima_data = pd.read_csv('UCIdata-exercise1/pima-indians-diabetes.data', names = col_names)\n",
    "for col in ['Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age']:\n",
    "    val = pima_data[col].mean()\n",
    "    pima_data[col] = pima_data[col].replace(0, val)\n",
    "\n",
    "#firstly we will seperate 2 classes and its respective features. We will create 2 different arrays. Each element of every array\n",
    "#will include 8 dimensions\n",
    "yes_diabetes=pima_data[pima_data[\"Class\"] == 1].drop([\"Class\"], axis=1).to_numpy()\n",
    "no_diabetes=pima_data[pima_data[\"Class\"] == 0].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "#yes_diabetes\n",
    "Φ=yes_diabetes\n",
    "mean43=(np.sum(Φ,axis=0)/yes_diabetes.shape[0]).tolist() #case I.I.D dimensions\n",
    "Σ43=[]\n",
    "for column in range(yes_diabetes.shape[1]):\n",
    "    Σ43.append(np.var(yes_diabetes[:,column]))\n",
    "\n",
    "def yes_diabetes_pdf_3 (x): #x be a 4-dimensional vector\n",
    "    pdf= pdf_naive(x,mean43,Σ43)\n",
    "    return pdf\n",
    "\n",
    "#no_diabetes\n",
    "Φ=no_diabetes\n",
    "mean53=(np.sum(Φ,axis=0)/no_diabetes.shape[0]).tolist() #case I.I.D dimensions\n",
    "Σ53=[]\n",
    "for column in range(no_diabetes.shape[1]):\n",
    "    Σ53.append(np.var(no_diabetes[:,column]))\n",
    "\n",
    "def no_diabetes_pdf_3 (x): #x be a 4-dimensional vector\n",
    "    pdf= pdf_naive(x,mean53,Σ53)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a-24UtBVh8q"
   },
   "source": [
    "#### 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqRDqou2Us-H"
   },
   "outputs": [],
   "source": [
    "# We will create a function that returns PDF using the Naive Bayes approach,when marginal pdfs are computed using \n",
    "#1-d Parzen windows with gaussian kernels \n",
    "#Since marginal pdfs are computed using 1-d Parzen windows with gaussian kernels, for the coputation of the PDF of each\n",
    "#dimension We will use for each dimension the average of n Gaussian functions with each data point as a center. σ needs\n",
    "#to be predetermined and will be the width h of each parzen window. Then, since naive bayes approach is used, the pdf of \n",
    "#each dimension will be multiplied in order to get the final pdf \n",
    "\n",
    "\n",
    "def kde_parzen_pdf_naive_bayes(x_kde,x_vector,bw): #x_kde an array with columns as dimensions whose pdfs should be calculated.\n",
    "    #Note that this function will calculate the pdf of each array seperately using the parzen - windows method with h=bw,\n",
    "    # and then it will calculate the pdf of the whole array 1-d vector with dimensions as many as the columns of the array x_kde\n",
    "    #using the naive bayes approach\n",
    "    pdf=1\n",
    "    for dimension in range(x_kde.shape[1]):\n",
    "        sum11=0\n",
    "        for sample in range (x_kde.shape[0]):\n",
    "            sum11+=norm(x_kde[sample,dimension],bw).pdf(x_vector[dimension])\n",
    "        pdf=pdf*sum11/x_kde.shape[0]        \n",
    "    return pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ohcW_1zUs-H"
   },
   "outputs": [],
   "source": [
    "# Firstly for the iris data \n",
    "col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Class']\n",
    "iris_data = pd.read_csv('UCIdata-exercise1/iris.data', names = col_names)\n",
    "\n",
    "#firstly we will seperate 3 classes and its respective features. We will create 3 different arrays. Each element of every array\n",
    "#will include 4 dimensions\n",
    "setosa=iris_data[iris_data[\"Class\"] == 'Iris-setosa'].drop([\"Class\"], axis=1).to_numpy()\n",
    "versicolor=iris_data[iris_data[\"Class\"] == 'Iris-versicolor'].drop([\"Class\"], axis=1).to_numpy()\n",
    "virginica=iris_data[iris_data[\"Class\"] == 'Iris-virginica'].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "#h will be the same for all four dimensions of all three differenct classes. We have three different patterns \n",
    "#observed in the iris data (setosa, versicolor and virginica) so the h will be the squareroot of 3\n",
    "h=3**(1/2)\n",
    "\n",
    "def setosa_pdf_4(x):\n",
    "    return kde_parzen_pdf_naive_bayes(setosa,x,h)\n",
    "    \n",
    "def versicolor_pdf_4(x):\n",
    "    return kde_parzen_pdf_naive_bayes(versicolor,x,h)\n",
    "\n",
    "def virginica_pdf_4(x):\n",
    "    return kde_parzen_pdf_naive_bayes(virginica,x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMQmAj7xUs-I"
   },
   "outputs": [],
   "source": [
    "# Then for the pima data\n",
    "col_names = ['Pregnancies','Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age','Class']\n",
    "pima_data = pd.read_csv('UCIdata-exercise1/pima-indians-diabetes.data', names = col_names)\n",
    "for col in ['Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age']:\n",
    "    val = pima_data[col].mean()\n",
    "    pima_data[col] = pima_data[col].replace(0, val)\n",
    "\n",
    "#firstly we will seperate 2 classes and its respective features. We will create 2 different arrays. Each element of every array\n",
    "#will include 8 dimensions\n",
    "yes_diabetes=pima_data[pima_data[\"Class\"] == 1].drop([\"Class\"], axis=1).to_numpy()\n",
    "no_diabetes=pima_data[pima_data[\"Class\"] == 0].drop([\"Class\"], axis=1).to_numpy()\n",
    "\n",
    "#h will be the same for all 8 dimensions of all 2 differenct classes. We have 2 different patterns \n",
    "#observed in the pima data (1,0) so the h will be the squareroot of 2\n",
    "h=2**(1/2)\n",
    "\n",
    "#yes_diabetes\n",
    "def yes_diabetes_pdf_4(x):\n",
    "    return kde_parzen_pdf_naive_bayes(yes_diabetes,x,h)\n",
    "\n",
    "#no_diabetes\n",
    "def no_diabetes_pdf_4(x):\n",
    "    return kde_parzen_pdf_naive_bayes(no_diabetes,x,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIBAwg0yUs-I"
   },
   "source": [
    " AIC and BIC penalizes the usage of many parameters for possible overfitting. So lets calculate the parameters used for every model  and the log likelihood for all models and all classes. Remember that the best model for each class is the one that has achieved the lowest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOE-B06DUs-I"
   },
   "outputs": [],
   "source": [
    "# Firstly for the iris_dataset\n",
    "#All three classes have a length of train dataset n=50. The first method, has k=5 parameters (four different means and one \n",
    "#different parameter in the Covariance Matrix). The second method has k=20 parameters (four different means and 16 different\n",
    "#parametrs in the covariance matrix) The third method has k=8 parameters (4 different means and four differnet parameters - four \n",
    "#different variances of each gaussian distibution). The fourth method is non parametric with k=1, only the width of the parzen\n",
    "#window. So for the first three methods, since n/k<40, AICc method will be used, and for the fourth AIC will be used.\n",
    "\n",
    "setosa_AIC={}\n",
    "versicolor_AIC={}\n",
    "virginica_AIC={}\n",
    "setosa_BIC={}\n",
    "versicolor_BIC={}\n",
    "virginica_BIC={}\n",
    "n=50\n",
    "\n",
    "#2a)\n",
    "#setosa\n",
    "Π=1\n",
    "k=5\n",
    "for count in range(len(setosa)):\n",
    "    Π=Π*setosa_pdf_1(setosa[count])\n",
    "setosa_AIC['2a']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "setosa_BIC['2a']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#versicolor\n",
    "Π=1\n",
    "k=5\n",
    "for count in range(len(versicolor)):\n",
    "    Π=Π*versicolor_pdf_1(versicolor[count])\n",
    "versicolor_AIC['2a']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "versicolor_BIC['2a']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#virginica\n",
    "Π=1\n",
    "k=5\n",
    "for count in range(len(virginica)):\n",
    "    Π=Π*virginica_pdf_1(virginica[count])\n",
    "virginica_AIC['2a']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "virginica_BIC['2a']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#2b)\n",
    "#setosa\n",
    "Π=1\n",
    "k=20\n",
    "for count in range(len(setosa)):\n",
    "    Π=Π*setosa_pdf_2(setosa[count])\n",
    "setosa_AIC['2b']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "setosa_BIC['2b']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#versicolor\n",
    "Π=1\n",
    "k=20\n",
    "for count in range(len(versicolor)):\n",
    "    Π=Π*versicolor_pdf_2(versicolor[count])\n",
    "versicolor_AIC['2b']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "versicolor_BIC['2b']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#virginica\n",
    "Π=1\n",
    "k=20\n",
    "for count in range(len(virginica)):\n",
    "    Π=Π*virginica_pdf_2(virginica[count])\n",
    "virginica_AIC['2b']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "virginica_BIC['2b']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "\n",
    "#2c)\n",
    "#setosa\n",
    "Π=1\n",
    "k=8\n",
    "for count in range(len(setosa)):\n",
    "    Π=Π*setosa_pdf_3(setosa[count])\n",
    "setosa_AIC['2c']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "setosa_BIC['2c']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#versicolor\n",
    "Π=1\n",
    "k=8\n",
    "for count in range(len(versicolor)):\n",
    "    Π=Π*versicolor_pdf_3(versicolor[count])\n",
    "versicolor_AIC['2c']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "versicolor_BIC['2c']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#virginica\n",
    "Π=1\n",
    "k=8\n",
    "for count in range(len(virginica)):\n",
    "    Π=Π*virginica_pdf_3(virginica[count])\n",
    "virginica_AIC['2c']=-2*math.log(Π)+2*k+(2*k+1)/(n-k-1)\n",
    "virginica_BIC['2c']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#2d)\n",
    "#setosa\n",
    "Π=1\n",
    "k=1\n",
    "for count in range(len(setosa)):\n",
    "    Π=Π*setosa_pdf_4(setosa[count])\n",
    "setosa_AIC['2d']=-2*math.log(Π)+2*k\n",
    "setosa_BIC['2d']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#versicolor\n",
    "Π=1\n",
    "k=8\n",
    "for count in range(len(versicolor)):\n",
    "    Π=Π*versicolor_pdf_4(versicolor[count])\n",
    "versicolor_AIC['2d']=-2*math.log(Π)+2*k\n",
    "versicolor_BIC['2d']=-2*math.log(Π)+k*math.log(n)\n",
    "\n",
    "#virginica\n",
    "Π=1\n",
    "k=8\n",
    "for count in range(len(virginica)):\n",
    "    Π=Π*virginica_pdf_4(virginica[count])\n",
    "virginica_AIC['2d']=-2*math.log(Π)+2*k\n",
    "virginica_BIC['2d']=-2*math.log(Π)+k*math.log(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ujQ_2AaUs-J"
   },
   "outputs": [],
   "source": [
    "# Then for the pima dataset\n",
    "#The two classes (yes_diabetes - no_diabetes) have a length of train dataset n1=268 and n2=500 respectively. The first method, \n",
    "#has k=9 parameters(8 different means and one different parameter in the Covariance Matrix). The second method has k=72 \n",
    "#parameters (8 different means and 64 different parameters in the covariance matrix) \n",
    "#The third method has k=16 parameters (8 different means 8 four differnet parameters - 8 different variances of each \n",
    "#gaussian distibution). The fourth method is non parametric with k=1, only the width of the parzen\n",
    "#window. So for the first three methods (except method 2a for no_diabetes), since n/k<40, AICc method will be used, \n",
    "#and for the fourth AIC will be used.\n",
    "\n",
    "yes_diabetes_AIC={}\n",
    "no_diabetes_AIC={}\n",
    "yes_diabetes_BIC={}\n",
    "no_diabetes_BIC={}\n",
    "n1=268\n",
    "n2=500\n",
    "\n",
    "#2a)\n",
    "k=9\n",
    "\n",
    "#no_diabetes\n",
    "Π=0\n",
    "for count in range(len(no_diabetes)):\n",
    "    Π+=math.log(no_diabetes_pdf_1(no_diabetes[count]))\n",
    "no_diabetes_AIC['2a']=-2*Π+2*k\n",
    "no_diabetes_BIC['2a']=-2*Π+k*math.log(n2)\n",
    "\n",
    "#yes_diabetes\n",
    "Π=0\n",
    "for count in range(len(yes_diabetes)):\n",
    "     Π+=math.log(yes_diabetes_pdf_1(yes_diabetes[count]))\n",
    "yes_diabetes_AIC['2a']=-2*Π+2*k+(2*k+1)/(n1-k-1)\n",
    "yes_diabetes_BIC['2a']=-2*Π+k*math.log(n1)\n",
    "\n",
    "\n",
    "#2b)\n",
    "k=72\n",
    "\n",
    "#no_diabetes\n",
    "Π=0\n",
    "for count in range(len(no_diabetes)):\n",
    "    Π+=math.log(no_diabetes_pdf_2(no_diabetes[count]))\n",
    "no_diabetes_AIC['2b']=-2*Π+2*k+(2*k+1)/(n1-k-1)\n",
    "no_diabetes_BIC['2b']=-2*Π+k*math.log(n2)\n",
    "\n",
    "#yes_diabetes\n",
    "Π=0\n",
    "for count in range(len(yes_diabetes)):\n",
    "     Π+=math.log(yes_diabetes_pdf_2(yes_diabetes[count]))\n",
    "yes_diabetes_AIC['2b']=-2*Π+2*k+(2*k+1)/(n1-k-1)\n",
    "yes_diabetes_BIC['2b']=-2*Π+k*math.log(n1)\n",
    "\n",
    "\n",
    "#2c)\n",
    "k=16\n",
    "\n",
    "#no_diabetes\n",
    "Π=0\n",
    "for count in range(len(no_diabetes)):\n",
    "    Π+=math.log(no_diabetes_pdf_3(no_diabetes[count]))\n",
    "no_diabetes_AIC['2c']=-2*Π+2*k+(2*k+1)/(n1-k-1)\n",
    "no_diabetes_BIC['2c']=-2*Π+k*math.log(n2)\n",
    "\n",
    "#yes_diabetes\n",
    "Π=0\n",
    "for count in range(len(yes_diabetes)):\n",
    "     Π+=math.log(yes_diabetes_pdf_3(yes_diabetes[count]))\n",
    "yes_diabetes_AIC['2c']=-2*Π+2*k+(2*k+1)/(n1-k-1)\n",
    "yes_diabetes_BIC['2c']=-2*Π+k*math.log(n1)\n",
    "\n",
    "\n",
    "#2d)\n",
    "k=1\n",
    "\n",
    "#no_diabetes\n",
    "Π=0\n",
    "for count in range(len(no_diabetes)):\n",
    "    Π+=math.log(no_diabetes_pdf_4(no_diabetes[count]))\n",
    "no_diabetes_AIC['2d']=-2*Π+2*k\n",
    "no_diabetes_BIC['2d']=-2*Π+k*math.log(n2)\n",
    "\n",
    "#yes_diabetes\n",
    "Π=0\n",
    "for count in range(len(yes_diabetes)):\n",
    "     Π+=math.log(yes_diabetes_pdf_4(yes_diabetes[count]))\n",
    "yes_diabetes_AIC['2d']=-2*Π+2*k\n",
    "yes_diabetes_BIC['2d']=-2*Π+k*math.log(n1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaW2FE3QUs-J",
    "outputId": "4b8bbb83-116b-4f2b-a18d-14b08deca252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AIC score for Class No_DiAbetes is:  {'2a': 42855.42917658481, '2b': 27258.012228253254, '2c': 27853.19067491427, '2d': 26126.26832363787} \n",
      "\n",
      "The BIC score for Class No_DiAbetes is:  {'2a': 42893.36064947061, '2b': 27560.720421596063, '2c': 27920.492930385437, '2d': 26130.482931736293} \n",
      "\n",
      "The AIC score for Class Yes_DiAbetes is:  {'2a': 24480.971847085468, '2b': 15373.93813892561, '2c': 15507.527366447659, '2d': 14259.57978071183} \n",
      "\n",
      "The BIC score for Class Yes_DiAbetes is:  {'2a': 24513.217086499215, '2b': 15631.745611778802, '2c': 15564.851684032246, '2d': 14263.170767692342} \n",
      "\n",
      "The AIC score for Class setosa is:  {'2a': 94.94668240182668, '2b': -46.79638879069642, '2c': -19.26144711916602, '2d': 522.7815751781123} \n",
      "\n",
      "The BIC score for Class setosa is:  {'2a': 104.2567974289674, '2b': -9.969721785581783, '2c': -4.379897222082317, '2d': 524.6935981835405} \n",
      "\n",
      "The AIC score for Class virginica is:  {'2a': 302.53967708918515, '2b': 158.59574100887042, '2c': 232.8430904010636, '2d': 561.5023091432291} \n",
      "\n",
      "The BIC score for Class virginica is:  {'2a': 311.8497921163259, '2b': 195.42240801398506, '2c': 247.7246402981473, '2d': 576.7984931866542} \n",
      "\n",
      "The AIC score for Class versicolor is:  {'2a': 226.51560783100473, '2b': 61.23241290948919, '2c': 160.3223257565067, '2d': 550.7735504171225} \n",
      "\n",
      "The BIC score for Class versicolor is:  {'2a': 235.82572285814547, '2b': 98.05907991460383, '2c': 175.20387565359042, '2d': 566.0697344605476} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('The AIC score for Class No_DiAbetes is: ',no_diabetes_AIC,'\\n')\n",
    "print ('The BIC score for Class No_DiAbetes is: ',no_diabetes_BIC,'\\n')\n",
    "print ('The AIC score for Class Yes_DiAbetes is: ',yes_diabetes_AIC,'\\n')\n",
    "print ('The BIC score for Class Yes_DiAbetes is: ',yes_diabetes_BIC,'\\n')\n",
    "print ('The AIC score for Class setosa is: ',setosa_AIC,'\\n')\n",
    "print ('The BIC score for Class setosa is: ',setosa_BIC,'\\n')\n",
    "print ('The AIC score for Class virginica is: ' ,virginica_AIC,'\\n')\n",
    "print ('The BIC score for Class virginica is: ',virginica_BIC,'\\n')\n",
    "print ('The AIC score for Class versicolor is: ',versicolor_AIC,'\\n')\n",
    "print ('The BIC score for Class versicolor is: ',versicolor_BIC,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWodvCbXUs-J"
   },
   "source": [
    "So as shown above, both AIC and BIC has concluded that the best models for predicting each class of iris data are ranked as following:\n",
    "\n",
    "1st) Model 2b\n",
    "\n",
    "2nd) Model 2c\n",
    "\n",
    "3rd) Model 2a\n",
    "\n",
    "4th) Model 2d\n",
    "\n",
    "\n",
    "For pima data, the best models for predicting each class of iris data are ranked as follwing:\n",
    "\n",
    "1st) Model 2d\n",
    "\n",
    "2nd) Model 2b\n",
    "\n",
    "3rd) Model 2c\n",
    "\n",
    "4th) Model 2a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsaXaubLVqVK"
   },
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhkQNSIzUs-K",
    "outputId": "0acc99e9-acfd-431d-c069-7505cc09d3a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of implementing the 3a algorithm and a 10-fold cross validation is:  92.0 %\n",
      "\n",
      "Accuracy of implementing the 3b algorithm and a 10-fold cross validation is:  98.0 %\n",
      "\n",
      "Accuracy of implementing the 3c algorithm and a 10-fold cross validation is:  94.66666666666667 %\n",
      "\n",
      "Accuracy of implementing the 3d algorithm and a 10-fold cross validation is:  92.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Firstly for the iris_data\n",
    "#Since from the initial iris_data each class appears 50 times, we conclude that the 3 classes are equi- probable. Therefore, \n",
    "#each time the classification of each datapoint of the test-data array will be performed only by calculating the greatest pdf\n",
    "#respectively. Α 10-fold cross-validation will be implemented.\n",
    "\n",
    "col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Class']\n",
    "iris_data = pd.read_csv('UCIdata-exercise1/iris.data', names = col_names)\n",
    "\n",
    "splited=partition(iris_data,10)\n",
    "accuracy_list_3a=[]\n",
    "accuracy_list_3b=[]\n",
    "accuracy_list_3c=[]\n",
    "accuracy_list_3d=[]\n",
    "\n",
    "\n",
    "for i in range(len(splited)):\n",
    "    splited_2=splited[:]\n",
    "    test_data=splited_2.pop(i)\n",
    "    train_data=pd.concat(splited_2)\n",
    "    train_data.index=range(len(train_data))\n",
    "    test_data.index=range(len(test_data))\n",
    "    #Now we will create the three classes solely from the train_data of this iteration. From these classes each time\n",
    "    #the pdf will be computed\n",
    "    setosa=train_data[train_data[\"Class\"] == 'Iris-setosa'].drop([\"Class\"], axis=1).to_numpy()\n",
    "    versicolor=train_data[train_data[\"Class\"] == 'Iris-versicolor'].drop([\"Class\"], axis=1).to_numpy()\n",
    "    virginica=train_data[train_data[\"Class\"] == 'Iris-virginica'].drop([\"Class\"], axis=1).to_numpy()\n",
    "    \n",
    "#############################################  \n",
    "    #pdf for a) case\n",
    "    # setosa Class\n",
    "    Φ=setosa\n",
    "    mean11=np.sum(Φ,axis=0)/setosa.shape[0]\n",
    "    var=0\n",
    "    for column in range(setosa.shape[1]):\n",
    "        if np.var(setosa[:,column])>var:\n",
    "            var=np.var(setosa[:,column])\n",
    "    Σ11=np.identity(setosa.shape[1])*var    #calculate the covariaance matrix\n",
    "    def setosa_pdf_3a (x): #x be a 4-dimensional vector\n",
    "        pdf= multivariate_normal(mean11,Σ11).pdf(x)\n",
    "        return pdf\n",
    "\n",
    "    # versicolor Class\n",
    "    Φ=versicolor\n",
    "    mean21=np.sum(Φ,axis=0)/versicolor.shape[0] \n",
    "    var=0\n",
    "    for column in range(versicolor.shape[1]):\n",
    "        if np.var(versicolor[:,column])>var:\n",
    "            var=np.var(versicolor[:,column])\n",
    "    Σ21=np.identity(versicolor.shape[1])*var    #calculate the covariaance matrix\n",
    "    def versicolor_pdf_3a (x): #x be a 4-dimensional vector\n",
    "        pdf= multivariate_normal(mean21,Σ21).pdf(x)\n",
    "        return pdf\n",
    "\n",
    "    # virginica Class\n",
    "    Φ=virginica\n",
    "    mean31=np.sum(Φ,axis=0)/virginica.shape[0] \n",
    "    var=0\n",
    "    for column in range(virginica.shape[1]):\n",
    "        if np.var(virginica[:,column])>var:\n",
    "            var=np.var(virginica[:,column])\n",
    "    Σ31=np.identity(virginica.shape[1])*var    #calculate the covariaance matrix\n",
    "    def virginica_pdf_3a (x): #x be a 4-dimensional vector\n",
    "        pdf= multivariate_normal(mean31,Σ31).pdf(x)\n",
    "        return pdf\n",
    "    \n",
    "###################################################3    \n",
    "    #pdf for b) case\n",
    "    # setosa\n",
    "    Φ=setosa\n",
    "    mean12=np.sum(Φ,axis=0)/setosa.shape[0] #case I.I.D dimensions\n",
    "    sum_=0\n",
    "    for i in range(setosa.shape[0]):\n",
    "        sum_+=np.matmul((Φ[i]-mean12).reshape(4,1),np.transpose((Φ[i]-mean12).reshape(4,1)))\n",
    "    Σ12=sum_/setosa.shape[0]\n",
    "    def setosa_pdf_3b (x): #x be a 4-dimensional vector\n",
    "        pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean12).reshape(4,1)),np.linalg.inv(Σ12)),(x-mean12).reshape(4,1)))/(((2*math.pi)**(setosa.shape[1]/2))*(np.linalg.det(Σ12))**(1/2))\n",
    "        return pdf\n",
    "\n",
    "    # versicolor\n",
    "    Φ=versicolor\n",
    "    mean22=np.sum(Φ,axis=0)/versicolor.shape[0] #case I.I.D dimensions\n",
    "    sum_=0\n",
    "    for i in range(versicolor.shape[0]):\n",
    "        sum_+=np.matmul((Φ[i]-mean22).reshape(4,1),np.transpose((Φ[i]-mean22).reshape(4,1)))\n",
    "    Σ22=sum_/versicolor.shape[0]\n",
    "    def versicolor_pdf_3b (x): #x be a 4-dimensional vector\n",
    "        pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean22).reshape(4,1)),np.linalg.inv(Σ22)),(x-mean22).reshape(4,1)))/(((2*math.pi)**(versicolor.shape[1]/2))*(np.linalg.det(Σ22))**(1/2))\n",
    "        return pdf\n",
    "\n",
    "    # virginica\n",
    "    Φ=virginica\n",
    "    mean32=np.sum(Φ,axis=0)/virginica.shape[0] #case I.I.D dimensions\n",
    "    sum_=0\n",
    "    for i in range(virginica.shape[0]):\n",
    "        sum_+=np.matmul((Φ[i]-mean32).reshape(4,1),np.transpose((Φ[i]-mean32).reshape(4,1)))\n",
    "    Σ32=sum_/virginica.shape[0]\n",
    "    def virginica_pdf_3b (x): #x be a 4-dimensional vector\n",
    "        pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean32).reshape(4,1)),np.linalg.inv(Σ32)),(x-mean32).reshape(4,1)))/(((2*math.pi)**(virginica.shape[1]/2))*(np.linalg.det(Σ32))**(1/2))\n",
    "        return pdf\n",
    "\n",
    "###################################################3    \n",
    "    #pdf for c) case\n",
    "    # setosa\n",
    "    Φ=setosa\n",
    "    mean13=(np.sum(Φ,axis=0)/setosa.shape[0]).tolist() #case I.I.D dimensions\n",
    "    Σ13=[]\n",
    "    for column in range(setosa.shape[1]):\n",
    "        Σ13.append(np.var(setosa[:,column]))\n",
    "\n",
    "    def setosa_pdf_3c (x): #x be a 4-dimensional vector\n",
    "        pdf= pdf_naive(x,mean13,Σ13)\n",
    "        return pdf\n",
    "\n",
    "    #versicolor\n",
    "    Φ=versicolor\n",
    "    mean23=(np.sum(Φ,axis=0)/versicolor.shape[0]).tolist() #case I.I.D dimensions\n",
    "    Σ23=[]\n",
    "    for column in range(versicolor.shape[1]):\n",
    "        Σ23.append(np.var(versicolor[:,column]))\n",
    "\n",
    "    def versicolor_pdf_3c (x): #x be a 4-dimensional vector\n",
    "        pdf= pdf_naive(x,mean23,Σ23)\n",
    "        return pdf\n",
    "\n",
    "    #virginica\n",
    "    Φ=virginica\n",
    "    mean33=(np.sum(Φ,axis=0)/virginica.shape[0]).tolist() #case I.I.D dimensions\n",
    "    Σ33=[]\n",
    "    for column in range(virginica.shape[1]):\n",
    "        Σ33.append(np.var(virginica[:,column]))\n",
    "\n",
    "    def virginica_pdf_3c (x): #x be a 4-dimensional vector\n",
    "        pdf= pdf_naive(x,mean33,Σ33)\n",
    "        return pdf\n",
    "    \n",
    "##################################################\n",
    "    #pdf for d) case    \n",
    "    h=3**(1/2)\n",
    "\n",
    "    def setosa_pdf_3d(x):\n",
    "        return kde_parzen_pdf_naive_bayes(setosa,x,h)\n",
    "    \n",
    "    def versicolor_pdf_3d(x):\n",
    "        return kde_parzen_pdf_naive_bayes(versicolor,x,h)\n",
    "\n",
    "    def virginica_pdf_3d(x):\n",
    "        return kde_parzen_pdf_naive_bayes(virginica,x,h)   \n",
    "    \n",
    "###################################################\n",
    "#starting the counting of correct predictions of each method for this iteration\n",
    "\n",
    "    counter_accuracy_3a=0\n",
    "    counter_accuracy_3b=0\n",
    "    counter_accuracy_3c=0\n",
    "    counter_accuracy_3d=0\n",
    "    \n",
    "    for row in range (len(test_data)):\n",
    "        #method 3a\n",
    "        pred1=setosa_pdf_3a(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred2=versicolor_pdf_3a(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred3=virginica_pdf_3a(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if max(pred1,pred2,pred3)==pred1:\n",
    "            if test_data.loc[row]['Class']=='Iris-setosa':\n",
    "                counter_accuracy_3a+=1\n",
    "        elif max(pred1,pred2,pred3)==pred2:\n",
    "            if test_data.loc[row]['Class']=='Iris-versicolor':\n",
    "                counter_accuracy_3a+=1      \n",
    "        else:\n",
    "            if test_data.loc[row]['Class']=='Iris-virginica':\n",
    "                counter_accuracy_3a+=1    \n",
    "        \n",
    "        #method 3b\n",
    "        pred1=setosa_pdf_3b(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred2=versicolor_pdf_3b(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred3=virginica_pdf_3b(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if max(pred1,pred2,pred3)==pred1:\n",
    "            if test_data.loc[row]['Class']=='Iris-setosa':\n",
    "                counter_accuracy_3b+=1\n",
    "        elif max(pred1,pred2,pred3)==pred2:\n",
    "            if test_data.loc[row]['Class']=='Iris-versicolor':\n",
    "                counter_accuracy_3b+=1      \n",
    "        else:\n",
    "            if test_data.loc[row]['Class']=='Iris-virginica':\n",
    "                counter_accuracy_3b+=1   \n",
    "        \n",
    "        #method 3c\n",
    "        pred1=setosa_pdf_3c(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred2=versicolor_pdf_3c(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred3=virginica_pdf_3c(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if max(pred1,pred2,pred3)==pred1:\n",
    "            if test_data.loc[row]['Class']=='Iris-setosa':\n",
    "                counter_accuracy_3c+=1\n",
    "        elif max(pred1,pred2,pred3)==pred2:\n",
    "            if test_data.loc[row]['Class']=='Iris-versicolor':\n",
    "                counter_accuracy_3c+=1      \n",
    "        else:\n",
    "            if test_data.loc[row]['Class']=='Iris-virginica':\n",
    "                counter_accuracy_3c+=1\n",
    "        \n",
    "        #method 3d\n",
    "        pred1=setosa_pdf_3d(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred2=versicolor_pdf_3d(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred3=virginica_pdf_3d(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if max(pred1,pred2,pred3)==pred1:\n",
    "            if test_data.loc[row]['Class']=='Iris-setosa':\n",
    "                counter_accuracy_3d+=1\n",
    "        elif max(pred1,pred2,pred3)==pred2:\n",
    "            if test_data.loc[row]['Class']=='Iris-versicolor':\n",
    "                counter_accuracy_3d+=1      \n",
    "        else:\n",
    "            if test_data.loc[row]['Class']=='Iris-virginica':\n",
    "                counter_accuracy_3d+=1  \n",
    "                  \n",
    "    accuracy_list_3a.append(counter_accuracy_3a/len(test_data))\n",
    "    accuracy_list_3b.append(counter_accuracy_3b/len(test_data))\n",
    "    accuracy_list_3c.append(counter_accuracy_3c/len(test_data))\n",
    "    accuracy_list_3d.append(counter_accuracy_3d/len(test_data))\n",
    "    \n",
    "final_accuracy_3a=mean(accuracy_list_3a)\n",
    "final_accuracy_3b=mean(accuracy_list_3b)\n",
    "final_accuracy_3c=mean(accuracy_list_3c)\n",
    "final_accuracy_3d=mean(accuracy_list_3d)\n",
    "\n",
    "print ('Accuracy of implementing the 3a algorithm and a 10-fold cross validation is: ',100*final_accuracy_3a,'%\\n')\n",
    "print ('Accuracy of implementing the 3b algorithm and a 10-fold cross validation is: ',100*final_accuracy_3b,'%\\n')\n",
    "print ('Accuracy of implementing the 3c algorithm and a 10-fold cross validation is: ',100*final_accuracy_3c,'%\\n')\n",
    "print ('Accuracy of implementing the 3d algorithm and a 10-fold cross validation is: ',100*final_accuracy_3d,'%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF3VHMCrUs-L"
   },
   "source": [
    "From the 2nd answer, it was shown both from AIC and BIC, that the best methods were at order: Method 2b, 2c 2a and 2d. This is mostly verified by our 10-fold cross validation since we have achieved the same ranking, except that method 3a and 3d have \n",
    "achieved the same accuracy (92 %). The 3b method has achieved a result of 98 % accuracy, which is slighlty better than our best\n",
    "NN classifier (8 NN as shown in exercise 1) which had averaged an accuracy of 96,44 % in 5-fold, 10-fold and 15-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sl7mgy16Us-L",
    "outputId": "0da8cadb-9e9b-4cd2-f784-cf9ac0a0c2c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of implementing the 3a algorithm and a 10-fold cross validation is:  65.63397129186603 %\n",
      "\n",
      "Accuracy of implementing the 3b algorithm and a 10-fold cross validation is:  76.56869446343131 %\n",
      "\n",
      "Accuracy of implementing the 3c algorithm and a 10-fold cross validation is:  74.87696514012305 %\n",
      "\n",
      "Accuracy of implementing the 3d algorithm and a 10-fold cross validation is:  82.29835953520164 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Then for the pima_data\n",
    "#For pima _data there are two seperate classes. From our original dataset there are 268 entries for class 1 (yes_diabetes)\n",
    "#and 500 entries for class 0 (no_diabetes). We consider the assumption that P(yes_diabetes)=0.35 and P(no_diabetes)=0.65\n",
    "#Therefore, each time the classification of each datapoint of the test-data array will be performed as following: \n",
    "#For each method, firstly, the pdf of each class for every entry of the dataset will be calculated.\n",
    "#Then this entry will be classified to class=0 (no_diabetes), if pdf(x|class=0)*0,65>pdf(x|class=1)*0.35, or else the \n",
    "#opposite classification will be performed.\n",
    "\n",
    "col_names = ['Pregnancies','Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age','Class']\n",
    "pima_data = pd.read_csv('UCIdata-exercise1/pima-indians-diabetes.data', names = col_names)\n",
    "for col in ['Glucose_Plasma','Blood Pressure','Skin THickness','Insulin','BMI','Diabetes Pedigree Function','Age']:\n",
    "    val = pima_data[col].mean()\n",
    "    pima_data[col] = pima_data[col].replace(0, val)\n",
    "\n",
    "splited=partition(pima_data,10)\n",
    "accuracy_list_3a=[]\n",
    "accuracy_list_3b=[]\n",
    "accuracy_list_3c=[]\n",
    "accuracy_list_3d=[]\n",
    "\n",
    "for i in range(len(splited)):\n",
    "    splited_2=splited[:]\n",
    "    test_data=splited_2.pop(i)\n",
    "    train_data=pd.concat(splited_2)\n",
    "    train_data.index=range(len(train_data))\n",
    "    test_data.index=range(len(test_data))\n",
    "    #Now we will create the two classes solely from the train_data of this iteration. From these classes each time\n",
    "    #the pdf will be computed\n",
    "    \n",
    "    yes_diabetes=pima_data[pima_data[\"Class\"] == 1].drop([\"Class\"], axis=1).to_numpy()\n",
    "    no_diabetes=pima_data[pima_data[\"Class\"] == 0].drop([\"Class\"], axis=1).to_numpy()\n",
    "    \n",
    "    #############################################  \n",
    "    #pdf for a) case\n",
    "    #yes_diabetes\n",
    "    Φ=yes_diabetes\n",
    "    mean41=np.sum(Φ,axis=0)/yes_diabetes.shape[0] #case I.I.D dimensions #case I.I.D dimensions\n",
    "    var=0\n",
    "    for column in range(yes_diabetes.shape[1]):\n",
    "        if np.var(yes_diabetes[:,column])>var:\n",
    "            var=np.var(yes_diabetes[:,column])\n",
    "    Σ41=np.identity(yes_diabetes.shape[1])*var    #calculate the covariaance matrix\n",
    "    def yes_diabetes_pdf_3a (x): #x be a 8-dimensional vector\n",
    "        pdf= multivariate_normal(mean41,Σ41).pdf(x)\n",
    "        return pdf\n",
    "\n",
    "    #no_diabetes\n",
    "    Φ=no_diabetes\n",
    "    mean51=np.sum(Φ,axis=0)/no_diabetes.shape[0] \n",
    "    var=0\n",
    "    for column in range(no_diabetes.shape[1]):\n",
    "        if np.var(no_diabetes[:,column])>var:\n",
    "            var=np.var(no_diabetes[:,column])\n",
    "    Σ51=np.identity(no_diabetes.shape[1])*var    #calculate the covariaance matrix\n",
    "    def no_diabetes_pdf_3a (x): #x be a 8-dimensional vector\n",
    "        pdf= multivariate_normal(mean51,Σ51).pdf(x)\n",
    "        return pdf\n",
    "    \n",
    "    ###################################################3    \n",
    "    #pdf for b) case\n",
    "    #yes_diabetes\n",
    "    Φ=yes_diabetes\n",
    "    mean42=np.sum(Φ,axis=0)/yes_diabetes.shape[0] #case I.I.D dimensions\n",
    "    sum_=0\n",
    "    for i in range(yes_diabetes.shape[0]):\n",
    "        sum_+=np.matmul((Φ[i]-mean42).reshape(8,1),np.transpose((Φ[i]-mean42).reshape(8,1)))\n",
    "    Σ42=sum_/yes_diabetes.shape[0]\n",
    "    def yes_diabetes_pdf_3b (x): #x be a 8-dimensional vector\n",
    "        pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean42).reshape(8,1)),np.linalg.inv(Σ42)),(x-mean42).reshape(8,1)))/(((2*math.pi)**(yes_diabetes.shape[1]/2))*(np.linalg.det(Σ42))**(1/2))\n",
    "        return pdf\n",
    "\n",
    "    #no_diabetes\n",
    "    Φ=no_diabetes\n",
    "    mean52=np.sum(Φ,axis=0)/no_diabetes.shape[0] #case I.I.D dimensions\n",
    "    sum_=0\n",
    "    for i in range(no_diabetes.shape[0]):\n",
    "        sum_+=np.matmul((Φ[i]-mean52).reshape(8,1),np.transpose((Φ[i]-mean52).reshape(8,1)))\n",
    "    Σ52=sum_/no_diabetes.shape[0]\n",
    "    def no_diabetes_pdf_3b (x): #x be a 8-dimensional vector\n",
    "        pdf= math.exp((-1/2)*np.matmul(np.matmul(np.transpose((x-mean52).reshape(8,1)),np.linalg.inv(Σ52)),(x-mean52).reshape(8,1)))/(((2*math.pi)**(no_diabetes.shape[1]/2))*(np.linalg.det(Σ52))**(1/2))\n",
    "        return pdf\n",
    "    \n",
    "     ###################################################3    \n",
    "    #pdf for c) case\n",
    "    #yes_diabetes\n",
    "    Φ=yes_diabetes\n",
    "    mean43=(np.sum(Φ,axis=0)/yes_diabetes.shape[0]).tolist() #case I.I.D dimensions\n",
    "    Σ43=[]\n",
    "    for column in range(yes_diabetes.shape[1]):\n",
    "        Σ43.append(np.var(yes_diabetes[:,column]))\n",
    "\n",
    "    def yes_diabetes_pdf_3c (x): #x be a 4-dimensional vector\n",
    "        pdf= pdf_naive(x,mean43,Σ43)\n",
    "        return pdf\n",
    "\n",
    "    #no_diabetes\n",
    "    Φ=no_diabetes\n",
    "    mean53=(np.sum(Φ,axis=0)/no_diabetes.shape[0]).tolist() #case I.I.D dimensions\n",
    "    Σ53=[]\n",
    "    for column in range(no_diabetes.shape[1]):\n",
    "        Σ53.append(np.var(no_diabetes[:,column]))\n",
    "\n",
    "    def no_diabetes_pdf_3c (x): #x be a 4-dimensional vector\n",
    "        pdf= pdf_naive(x,mean53,Σ53)\n",
    "        return pdf\n",
    "    \n",
    "    ###################################################3    \n",
    "    #pdf for d) case\n",
    "    #h will be the same for all 8 dimensions of all 2 differenct classes. We have 2 different patterns \n",
    "    #observed in the pima data (1,0) so the h will be the squareroot of 2\n",
    "    h=2**(1/2)\n",
    "\n",
    "    #yes_diabetes\n",
    "    def yes_diabetes_pdf_3d(x):\n",
    "        return kde_parzen_pdf_naive_bayes(yes_diabetes,x,h)\n",
    "\n",
    "    #no_diabetes\n",
    "    def no_diabetes_pdf_3d(x):\n",
    "        return kde_parzen_pdf_naive_bayes(no_diabetes,x,h)\n",
    "    \n",
    "    ###################################################\n",
    "#starting the counting of correct predictions of each method for this iteration\n",
    "\n",
    "    counter_accuracy_3a=0\n",
    "    counter_accuracy_3b=0\n",
    "    counter_accuracy_3c=0\n",
    "    counter_accuracy_3d=0\n",
    "    \n",
    "    for row in range (len(test_data)):\n",
    "        #method 3a\n",
    "        pred0=no_diabetes_pdf_3a(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred1=yes_diabetes_pdf_3a(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if pred0*0.65>pred1*0.35:\n",
    "            if test_data.loc[row]['Class']==0: #class 0 means no_diabetes - means chance 65 %\n",
    "                counter_accuracy_3a+=1\n",
    "        else:\n",
    "            if test_data.loc[row]['Class']==1:\n",
    "                counter_accuracy_3a+=1      \n",
    "        \n",
    "        #method 3b        \n",
    "        pred0=no_diabetes_pdf_3b(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred1=yes_diabetes_pdf_3b(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if pred0*0.65>pred1*0.35:\n",
    "            if test_data.loc[row]['Class']==0: #class 0 means no_diabetes - means chance 65 %\n",
    "                counter_accuracy_3b+=1\n",
    "        else:\n",
    "            if test_data.loc[row]['Class']==1:\n",
    "                counter_accuracy_3b+=1      \n",
    "        \n",
    "        #method 3c \n",
    "        pred0=no_diabetes_pdf_3c(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred1=yes_diabetes_pdf_3c(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if pred0*0.65>pred1*0.35:\n",
    "            if test_data.loc[row]['Class']==0: #class 0 means no_diabetes - means chance 65 %\n",
    "                counter_accuracy_3c+=1\n",
    "        else:\n",
    "            if test_data.loc[row]['Class']==1:\n",
    "                counter_accuracy_3c+=1  \n",
    "        \n",
    "        #method 3d \n",
    "        pred0=no_diabetes_pdf_3d(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        pred1=yes_diabetes_pdf_3d(test_data.drop([\"Class\"], axis=1).to_numpy()[row])\n",
    "        if pred0*0.65>pred1*0.35:\n",
    "            if test_data.loc[row]['Class']==0: #class 0 means no_diabetes - means chance 65 %\n",
    "                counter_accuracy_3d+=1\n",
    "        else:\n",
    "            if test_data.loc[row]['Class']==1:\n",
    "                counter_accuracy_3d+=1  \n",
    "                  \n",
    "    accuracy_list_3a.append(counter_accuracy_3a/len(test_data))\n",
    "    accuracy_list_3b.append(counter_accuracy_3b/len(test_data))\n",
    "    accuracy_list_3c.append(counter_accuracy_3c/len(test_data))\n",
    "    accuracy_list_3d.append(counter_accuracy_3d/len(test_data))\n",
    "    \n",
    "final_accuracy_3a=mean(accuracy_list_3a)\n",
    "final_accuracy_3b=mean(accuracy_list_3b)\n",
    "final_accuracy_3c=mean(accuracy_list_3c)\n",
    "final_accuracy_3d=mean(accuracy_list_3d)\n",
    "\n",
    "print ('Accuracy of implementing the 3a algorithm and a 10-fold cross validation is: ',100*final_accuracy_3a,'%\\n')\n",
    "print ('Accuracy of implementing the 3b algorithm and a 10-fold cross validation is: ',100*final_accuracy_3b,'%\\n')\n",
    "print ('Accuracy of implementing the 3c algorithm and a 10-fold cross validation is: ',100*final_accuracy_3c,'%\\n')\n",
    "print ('Accuracy of implementing the 3d algorithm and a 10-fold cross validation is: ',100*final_accuracy_3d,'%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJkeAVS0Us-M"
   },
   "source": [
    "From the part of the 2nd exercise when AIC and BIC were calculated, it was shown  that the best methods were at order: Method 2d, 2b 2c and 2a. This is verified by our 10-fold cross validation since we have achieved the same ranking, as shown above. The 3d method has achieved a result of 82.29 % accuracy, which is  better than our bestNN classifier (17 NN as shown in exercise 1) which had averaged an accuracy of 73.82 % in 10-fold ,20 - fold and 30-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fiN3qYmVw0P"
   },
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ2ZO2omUs-M"
   },
   "outputs": [],
   "source": [
    "# In order to examine whether the three classes of the iris_data are seperable by a hyperplane, we will perform the simple \n",
    "# perceptron algorithm. For this cause we will create the following function:\n",
    "\n",
    "def step_func(z):\n",
    "    return 1 if (z > 0) else 0\n",
    "\n",
    "def perceptron(X, y, ε, epochs):\n",
    "    \n",
    "    # X are the Inputs.\n",
    "    # y is labels/target (must consist of only 0s or 1s).\n",
    "    # ε is learning rate.\n",
    "    # epochs is the Number of iterations we want the algorithm to be executed\n",
    "    \n",
    "    # m is the number of training examples\n",
    "    # n is the number of features \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing parapeters(theta) to zeros.\n",
    "    # +1 in n+1 for the bias term.\n",
    "    theta = np.zeros((n+1,1))\n",
    "    \n",
    "    # Empty list to store how many examples were \n",
    "    # misclassified at every iteration.\n",
    "    n_miss_list = []\n",
    "    \n",
    "    # Training.\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # variable to store #misclassified.\n",
    "        n_miss = 0\n",
    "        \n",
    "        # looping for every example.\n",
    "        for idx, x_i in enumerate(X):\n",
    "            \n",
    "            # Insering 1 for bias, X0 = 1.\n",
    "            x_i = np.insert(x_i, 0, 1).reshape(-1,1)\n",
    "            \n",
    "            # Calculating prediction/hypothesis.\n",
    "            y_hat = step_func(np.dot(x_i.T, theta))\n",
    "            \n",
    "            # Updating if the example is misclassified.\n",
    "            if (np.squeeze(y_hat) - y[idx]) != 0:\n",
    "                theta =theta+ ε*((y[idx] - y_hat)*x_i)\n",
    "                \n",
    "                # Incrementing by 1.\n",
    "                n_miss += 1\n",
    "        \n",
    "        # Appending number of misclassified examples\n",
    "        # at every iteration.\n",
    "        n_miss_list.append(n_miss)\n",
    "        \n",
    "    return theta, n_miss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6drWrS4Us-M"
   },
   "outputs": [],
   "source": [
    "# There will be three examinations. For each examination, The X array will be the same. It will have length of 150, and \n",
    "#it will be compiled of 4 columns, each of them representing the dimensions that will be examined as inputs. Then, for each \n",
    "# run of the alforithm, three diferent y will be created. For example, for the examination of class setosa, y_setosa array \n",
    "# will be created in order to represent each setosa class as 1, and the other two classes as 0s.\n",
    "\n",
    "X=iris_data.to_numpy()[:,:4]\n",
    "y=iris_data.to_numpy()[:,4]\n",
    "y_setosa=pd.get_dummies(y)['Iris-setosa'].to_numpy()\n",
    "y_versicolor=pd.get_dummies(y)['Iris-versicolor'].to_numpy()\n",
    "y_virginica=pd.get_dummies(y)['Iris-virginica'].to_numpy()\n",
    "\n",
    "\n",
    "#Now we will run the algorithm for each class. We will implement a learning rate of 1 for all three runs,\n",
    "#10 epochs for each class setosa and 400 epochs for the other two classes versicolor and virginica\n",
    "theta_setosa, wrong_setosa=perceptron(X,y_setosa,1,10)\n",
    "\n",
    "#versicolor\n",
    "theta_versicolor, wrong_versicolor=perceptron(X,y_versicolor,1,400)\n",
    "\n",
    "#virginica\n",
    "theta_virginica, wrong_virginica=perceptron(X,y_virginica,1,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWgT0tHYUs-N",
    "outputId": "bdb5c181-c15d-4f6d-dcf6-0f9057655300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(wrong_setosa[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLNX1l8KUs-N"
   },
   "source": [
    "As shown above, in only ten epochs the algorithm converged, and managed to seperate class setosa from the other two classes. This means that in fact, setosa is seperable by a hyperplane from the other two classes and we can find an equation of a hyperplane in 4-d space that can fully seperate setosa from the other two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbw-DdlcUs-N",
    "outputId": "ef84d298-88dc-461f-d7c8-736f5f901c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(wrong_versicolor[-1])\n",
    "print(wrong_virginica[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k4XOjSIUs-N"
   },
   "source": [
    "On the other hand, both versicolor and virginica did not manage to converge, depsite the fact that the algorithm ran in 400 epochs each time. Indeed, since versicolor did not converge, and\n",
    " we showed that setosa converged, we expected that virginica would not converge as well. This is due to the fact that versicolor and virginica classes are mixed together and there is not a hyperplane in 4-d space that can seperate these two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5OoQ3CYUs-N",
    "outputId": "08f23ed4-7021-465f-a8db-de30041102fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(min(wrong_versicolor))\n",
    "print(min(wrong_virginica))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHJCCHevUs-O"
   },
   "source": [
    "As shown above, the best thing that the simple perceptron could do in 400 epochs, was for versicolor to create a hyperplane that would missclassify only 2 datapoints, and for virginica to create a hyperplane in 4-d space that would misclassify 1 datapoint"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW1_PROBLEM2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
